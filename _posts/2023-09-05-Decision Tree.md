---
title: 'Decision Tree'
date: 2023-09-05
permalink: /posts/2023/09/Decision Tree/
tags:
  - Machine Learning
  - Machine Learning Models
---
机器学习中的决策树模型和信息熵等知识。
## 信息熵
信息熵是度量样本集合纯度最常用的一种指标，假定样本集合$D$中第$K$类样本所占的比例为$p_k$，则样本集合$D$的信息熵为：
$$
\text{Ent}(D) = -\sum_{k=1}p_k\text{ln}p_k
$$
$\text{Ent}(D)$越小，$D$的纯度越高。
## 信息增益-ID3决策树
决策树中对离散属性$a$进行划分，假设得到$V$个分支结点，每个结点上有$D^v$个样本，给予每个分支结点赋予权重$|D^v|/|D|$，即样本数越多的分支结点的影响越大，则可以计算出属性$a$对样本进行划分后得到的信息增益：
$$
\text{Gain}(D,a) = \text{Ent}(D) - \sum_{v=1}^V\frac{|D^v|}{|D|}\text{Ent}(D^v)
$$
## 增益率-C4.5决策树
**信息增益准则对可取值较多的属性有所偏好，即属性取值越多，信息增益越大**。为了减少这种偏好带来的不利影响。使用增益率作为准则来选择划分属性：
$$
\text{Gainratio}(D,a)=\frac{\text{Gain}(D,a)}{\text{IV}(a)},\\
\text{IV} = -\sum_{v=1}^V\frac{|D^v|}{|D|}\text{ln}\frac{|D^v|}{|D|}
$$
**增益率准则可以对取值较少的属性有所偏好**，因此C4.5算法使用了启发式：先从候选的划分属性中找到信息增益高于平均水平的属性，再从中选出增益率最高的。
## 基尼指数-CART树
数据集D的纯度用基尼指数来度量，样本集合$D$中第$K$类样本所占的比例为$p_k$：
$$
\text{Gini}(D) = \sum_{k=1}\sum_{k'\neq k}p_kp_{k'}\\
=1-\sum_{k=1}p_k^2
$$
基尼指数反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，**Gini(D)越小，则数据集D的纯度越高。**
属性$a$的基尼指数定义为
$$
\text{Gini\_index}(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D^v)
$$
则在候选属性集合A中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。
## 决策树算法：
- ID3:以信息增益为准则来选择划分属性。
- C4.5：以增益率为准则来选择划分属性。
- CART：以基尼指数为准则来选择划分属性。
## 预剪枝
将训练数据划分出验证数据，对某节点某剩余属性，比较划分前后验证集的准确率，若划分后准确率提高，则继续划分，若没有提高，则将该节点作为叶子节点，占比最多的标签作为该叶子节点的标签。
## 后剪枝
对形成的决策树的结点从后往前剪枝，对比剪枝前后验证集的准确率是否提升，若提升则将该节点变为叶子节点，若没有提升保持不变。
