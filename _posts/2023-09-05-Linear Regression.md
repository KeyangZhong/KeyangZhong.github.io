---
title: 'Linear Regresssion'
date: 2023-09-05
permalink: /posts/2023/09/Linear Regresssion/
tags:
  - Machine Learning
  - Machine Learning Models
---
线性回处理的是回归问题，逻辑斯谛回归处理的是分类问题。

## 线性回归
线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记：
$$
f(x_i)=wx_i+b使得f(x_i)\equiv y_i
$$
可试图让均方误差最小化，通过最小二乘法求解，该方法试图找到一条直线，使得所有样本到直线上的欧式距离之和最小。
$$
(w^*,b^*) = \arg \min_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2 = \arg\min_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2
$$
有闭式解：
$$
w^* = \frac{\sum_{i=1}^{m}y_i(x_i-\overline{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}\\
b^* = \frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)
$$
对于矩阵形式，在$X^TX$可逆的情况下有闭式解：
$$
w^*=(X^TX)^{-1}X^Ty
$$

## 邻回归
最小二乘法要求各个特征之间要互相独立，保证$X^TX$可逆且满秩，若不满秩，则有多个解$w$，也就是即使可逆，如果特征之间有较大的多重共线性，也会使得其逆在数值山无法准确计算，即传统的最小二乘法缺乏稳定性和可靠性。

为了解决这个问题，提出来岭回归。给$X^TX$

的对角线都加上参数$λ$使得$X^TX+λI$满秩，通过损失无偏性来换取高的数值稳定性。
$$
w^*=(X^T+λI)^{-1}X^Ty
$$
岭回归看作是结构风险最小化准则下的最小二乘法估计，即加上了L2范数（L2正则化），其目标函数可以写为：
$$
L(w) = \frac{1}{2}||y-Xw||^2+\frac{1}{2}λ||w||^2_2
$$

## LASSO 回归
LASSO回归算法在损失函数加上了一个带惩罚系数λ的$w$向量的L1范数作为惩罚项：
$$
L(w) = \frac{1}{2}||y-Xw||^2+\frac{1}{2}λ||w||_1
$$
LASSO 可以防止过拟合并且产生稀疏解