---
title: 'Ensemble Learning'
date: 2023-09-07
permalink: /posts/2023/09/Ensemble Learning/
tags:
  - Machine Learning
  - Machine Learning Models
---
集成学习通过将多个学习器进行结合，获得比单一学习器显著优越的泛化性能。

参考：[【机器学习】Ensemble Learning 集成学习 + Python代码实战](http://t.csdn.cn/1nM4X)
## 集成学习概述
- Bagging： 训练多个分类器取平均
- Boosting: 从弱学习器开始加强，通过加权来进行训练。（加入一颗树，要比原来强）
## Bagging 
Bagging是一种并行式集成学习方法，首先采样出 个含 个训练样本的采样集，基于每个采样集训练出一个基学习
器，再将这些基学习器进行结合。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务
使用简单平均法。从偏差方差分析的角度看，Bagging主要关注降低方差。
- 全称：bootstrap aggregation
- 典型代表：随机森林（随机-数据随机采样，特征随机选择；森林-多个决策树并行放在一起。）
### 随机森林
随机森林在以决策树为基础学习器构建Bagging集成的基础上，进一步在决策树的训练过程引入了随机属性选择，对基决策树的每个节点，先从改节点的属性集合中随机选择包含k个属性的子集，然后再从这些子集中选择最优属性进行划分。

超参数k控制了随机性的引入程度，若令k=d，则基决策树的构建与传统决策树相同，若令k=1，则是随机选择一个属性用于划分，一般情况下令$k=long_2d$。

随机森林中的基学习器不仅来自Bagging带来的样本扰动，也来自本身的属性扰动。

## Boosting
提升方法Boosting是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列若分类器，将这些弱分类器线性组合，构成一个强分类器。

典型的提升方法是AdaBoost和GBDT。
### AdaBoost
AdaBoost算法是弱分类器的线性组合。
$$
f(x) = \sum_{m=1}^M\alpha_mG_m(x)
$$
AdaBoost算法通过每次迭代学习一个弱分类器，每次迭代中，提高哪些被前一轮分类器错误分类数据的权值，降低那些被正确分类数据的权值。最后AdaBoost将基本分类器线性组合成强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差大的分类器以小的权值。

**算法流程：** \
**输入：** 训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$,其中$x_i\in X \in\mathbb{R}^n, y_i\in Y={-1,1}$；\
**过程：**
1. 初始化训练数据权值分布$D_1=(w_{11},...,w_{1i},...,w_{1N}), w_{1i}=\frac{1}{N} \ i=1,2,...,N$
2. 对$m=1,2,...,M$
    1. 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$G_m(x):X\rightarrow \{-1,+1\} $
    2. 计算$G_m(x)$在训练数据集上的分类误差率：
    $$
    e_m = \sum_{i=1}^NP(G_m(x)\neq y_i) = \sum_{i=1}^Nw_{mi}I(G_m(x)\neq y_i)
    $$
    3. 计算$G_m(x)$的系数$\alpha_m=\frac{1}{2}\text{ln}\frac{1-e_m}{e_m}$
    4. 更新训练数据集的权值分布