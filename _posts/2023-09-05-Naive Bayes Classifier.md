---
title: 'Naive Bayes Classifier'
date: 2023-09-05
permalink: /posts/2023/09/Naive Bayes Classifier/
tags:
  - Machine Learning
  - Machine Learning Models
---
**朴素贝叶斯** **Naïve Bayes** 一种基于概率的生成模型。

## 全概率公式
其含义是如果有一些互斥的事件$B_1, ... ,B_n$，它们的并集为全集，则任何事件A发生的概率可以拆分为每一个$A\cap B_i $的概率之和。
$$
P(A) = \sum_{i=1}^n P(A|B_i)P(B_i)
$$


## 贝叶斯公式
贝叶斯本质就是一个条件概率公式 $P(B|A)$，即在A事件发生的概率下，B事件发生的概率。
$$
P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum_{j=1}^nP(A|B_j)P(B_j)}\\
=\frac{P(A|B_i)P(B_i)}{P(A)}
$$
## 先验概率
事情未发生，根据以往经验和分析得到的概率，在机器学习中是有大数据统计出来的（采样得到的概率）,如全概率公式，它往往作为"由因求果"问题中的"因"出现。

例1：数据集中共有140颗糖果，其中红色糖79颗，蓝色糖61颗，则类别的先验概率为 $P(C1) = 0.56, P(C1) = 0.44$。

例2： 常识认为世界上男女的比例大致为一比一，则男女类别的先验概率为0.5。
## 后验概率
后验概率是根据观察到的样本修正之后的概率值，事情已经发生，**已有结果，求引起这件事发生的因素的可能性。** $P(C1|X)$和$P(C2|X)$即为后验概率。\
后验概率的计算是以先验概率为前提条件的。如果只知道事情的结果，而不知道先验概率，是无法计算后验概率的。\
机器学习中，依据后验概率来给出样本分类结果。\
**最大后延概率（Maximum Posterior Probability）估计，朴素贝叶斯分类准则：后验概率最大化，哪个类别的后验概率最大，则该类别作为输出。**
## 条件独立假设
即用于分类的特征在确定的类别的条件下都是独立的：
$$
P(X=x|Y=c_k) = P(X^{(1)}=x^{1},...,X^{(n)}=x^{n}|Y=c_k)\\
=\prod_{i=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$
## 朴素贝叶斯法分类
$$
y = arg\max_{c_k} \frac{P(Y=c_k)*P(X|Y=c_k)}{\sum_k P(Y=c_i)P(X|Y=c_i)}\\
 = arg\max P(Y=c_k)*P(X|Y=c_k)
$$
## 朴素贝叶斯法的参数估计
### 极大似然估计
在朴素贝叶斯法中，学习意味着户籍$P(Y=c_k)和P(X^{(i)}=x^{(j)}|Y=c_k)$。\
先验概率$P(Y=c_k)$的极大似然估计是：
$$
P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N},k = 1,2 ...,K
$$
设第$j$个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j2},...,a_{jS_j}}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：
$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$
### 贝叶斯估计
用极大似然估计可能出现所要估计的概率值为0的情况。这是会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。条件概率的贝叶斯估计是
$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}\\
S_j是第j个特征可取的变量数
$$
式中$\lambda\geq0$。等价于在随机变量各个取值的频数上赋予一个正数。当$\lambda>0$时就是极大似然估计。常取$\lambda=1$，这时候称为拉普拉斯平滑。同样先验概率的贝叶斯估计是：
$$
P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{   N+K\lambda}\\
K是y的类别数
$$